{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center>On The Resemblance and Containment of Documents</center>\n",
    "#### <center> Andrei Z. Broder </center>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center>__Presented by Mike Mull (@kwikstep)__</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Resemblance r(A,B)\n",
    "\n",
    "- r(A,B) is a number between 0 (no resemblance) and 1 (essentially identical).\n",
    "  - d(A,B) = 1 - r(A,B) is a distance between documents\n",
    "  \n",
    "### Containment c(A,B)\n",
    "\n",
    "- c(A,B) also between 0 and 1.\n",
    "  - Here 1 means A is entirely contained in B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Broder's intention was to identify similar documents on the web as part of Digital's AltaVista search engine.  He notes that typical string similarity metrics like Levenshtein distance and Hamming distance would require comparing of entire documents, so instead he sets out to define resemblance and containment metrics that can be computed using only a _sketch_ of a document, basically a very abbreviated form that he goes on to define later.\n",
    "\n",
    "As is usual with similarity metrics, the value of these metrics is defined to go from 0 to 1.  Broder also notes that d = 1 - r is a true _metric_, ie it obeys the triangle inequality, so it's a good option for clustering algorithms.\n",
    "\n",
    "I'm going to mostly ignore the containment metric, although most of this discussion pertains to both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions as set intersection problems\n",
    "### Shingles (aka, N-Grams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'rose', 'is', 'a'),\n",
       " ('rose', 'is', 'a', 'rose'),\n",
       " ('is', 'a', 'rose', 'is'),\n",
       " ('a', 'rose', 'is', 'a'),\n",
       " ('rose', 'is', 'a', 'rose')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-gram function courtesy of Peter Norvig\n",
    "def ngrams(seq, n):\n",
    "    \"List all the (overlapping) ngrams in a sequence.\"\n",
    "    return (tuple(seq[i:i+n]) for i in range(1+len(seq)-n))\n",
    "\n",
    "doc1 = \"a rose is a rose is a rose\"\n",
    "list(ngrams(doc1.split(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The paper will define the resemblance and containment operation as set operations, but first it goes into detail on the composition of the sets.  Specifically, it defines a _shingle_ (aka, n-gram) as a sequence of overlapping tokens.  The set of interest then is the set of _sequences of tokens_ of length w.  The shingles capture some of the word order in the documents, which is important since the metrics they define are in terms of set intersections and unions, which don't care about order.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resemblance and Containment in Terms of Sets\n",
    "\n",
    "Suppose S(A,w) is the set of w-shingles for document A\n",
    "\n",
    "#### Resemblance\n",
    "$$\n",
    "r_w(A,B) = \\frac{|S(A,w) \\cap S(B,w)|}{|S(A,w) \\cup S(B,w)|}\n",
    "$$\n",
    "\n",
    "#### Containment\n",
    "\n",
    "$$\n",
    "c_w(A,B) = \\frac{|S(A,w) \\cap S(B,w)|}{|S(A,w)|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The resemblance metric is basically the size of the intersection of two sets of shingles divided by the size of the union. This metrics is also called _Jacccard similarity_. The containment metric is nearly the same except the numerator only includes the size of the set for a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    x = set(a)\n",
    "    y = set(b)\n",
    "    return len(x & y) / len(x | y)\n",
    "\n",
    "print(jaccard_similarity(ngrams('a rose is a rose is a rose'.split(), 1),\n",
    "                         ngrams('a rose is a flower which is a rose'.split(), 1)))\n",
    "\n",
    "print(jaccard_similarity(ngrams('a rose is a rose is a rose'.split(), 2),\n",
    "                         ngrams('a rose is a flower which is a rose'.split(), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Above is a simple implementation of the set resemblance function.  Note that the resemblance is lower for longer shingle lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shingles - Option B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'rose', 'is', 'a'),\n",
       " ('is', 'a', 'rose', 'is'),\n",
       " ('rose', 'is', 'a', 'rose')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blist = set(ngrams(doc1.split(), 4))\n",
    "blist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Broder defines two methods of creating the set of shingles.  Option B is the simpler (and more usual), however you lose a bit of information since duplicate shingles are collapsed.  Option A, which is shown below keeps an occurence number along with the shingle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shingles - Option A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 'rose', 'is', 'a'), 1),\n",
       " (('rose', 'is', 'a', 'rose'), 1),\n",
       " (('is', 'a', 'rose', 'is'), 1),\n",
       " (('a', 'rose', 'is', 'a'), 2),\n",
       " (('rose', 'is', 'a', 'rose'), 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "counts = defaultdict(int)\n",
    "alist = []\n",
    "for t in ngrams(doc1.split(), 4):\n",
    "    alist.append((t, counts[t]+1))\n",
    "    counts[t] += 1\n",
    "    \n",
    "alist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating the resemblance and containment\n",
    "\\begin{equation*}\n",
    " W \\subset \\Omega\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    " MIN_{s}(W) = \\mbox{the set of smallest s elements in W}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The next section discusses in detail how to _estimate_ the resemblance and containment.  The word estimate is important-- this is a technique based on probability and randomness, so it's not trying to compute these metric exactly.  Broder starts with two definitions.  First, he defines omega, which is the set of all shingles of size w.  As we'll see later the method doesn't really enumerate all possible shingles, but it's important for the descriptions below.\n",
    "\n",
    "Second, the function MINs(W) is defined, which is just basically the smallest s elements in the set W.  This implies that omega is totally ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theorem 1\n",
    "$$\n",
    "\\pi: \\Omega \\rightarrow \\Omega \\ \\ \\text{(permutation)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "M(A) = MIN_{s}(\\pi(S(A,k)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The paper then proceeds to the main theorem of the paper.  First, they define a permutation pi over the set of shingles, and then they define M(A), which is the minimum s elements of the permutation of the set of shingles for document A.  This function M(A) is what would later be called _MinHash_.  Given these preliminaries, theorem 1 states:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The value\n",
    "\n",
    "$$\n",
    "\\frac{|MIN_{s}(M(A) \\cup M(B)) \\cap M(A) \\cap M(B)|}{|MIN_{s}(M(A) \\cup M(B))|}\n",
    "$$\n",
    "\n",
    "is an _unbiased estimator_ of the resemblance of A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is the key idea in the paper.  I'm sure it's completely self-evident to everyone...just kidding.  First, for completeness let's define _unbiased estimator_.  This is a term from statistics that means that the expected value of this estimator (ie, the average) will approach the true population value as the number of cases gets large.  The important thing here is that this estimator is entirely in terms of the document \"sketches\".\n",
    "\n",
    "Now let's break down the parts of this expression. First, we'll look at the union term that's in both the numerator and denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "MIN_{s}(M(A) \\cup M(B)) & = MIN_{s}(MIN_{s}(\\pi(S(A,k)) \\cup MIN_s(\\pi(S(B,k)))) \\\\\n",
    " &= MIN_{s}(MIN_{s}(\\pi(S(A,k)) \\cup \\pi(S(B,k))) \\\\\n",
    " &= MIN_{s}(\\pi(S(A,k)) \\cup \\pi(S(B,k))) \\\\\n",
    " &= MIN_{s}(\\pi(S(A,k)) \\cup S(B,k)))\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is a result which is used in the proof of Theorem 1, although i've added some steps to (i hope) make it clearer.  This might still not be obvious, so the code below is a simple example that shows the equivalence in one case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'r', 't'] ['e', 'm', 'r']\n",
      "['e', 'm', 'r']\n",
      "['e', 'm', 'r']\n"
     ]
    }
   ],
   "source": [
    "def pi(x): return set({'a':'x', 'b':'m','c':'t','d':'e', 'e':'r'}[t] for t in x)\n",
    "def min_s(x, s): return sorted(x)[0:3]\n",
    "\n",
    "sA = {'a','b','c','e'}\n",
    "sB = {'a','b','d','e'}\n",
    "\n",
    "mA = min_s(pi(sA), 3)\n",
    "mB = min_s(pi(sB), 3)\n",
    "print(mA, mB)\n",
    "print(min_s(set(mA) | set(mB), 3))\n",
    "print(min_s(pi(sA | sB), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose there are 10 elements in the union of S(A,w) and S(B,w), and 6 elements in the intersection\n",
    "  - So the resemblance is 0.6\n",
    "- Another way to think of this is that if you choose an element at random from the union, there's a 60% chance it's also in the intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "M(A) \\cap M(B)\n",
    "$$\n",
    "\n",
    "__ Let &alpha; be the smallest element in__ $ \\pi(S(A,w)) \\cup S(B,w)) $\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Pr(\\alpha \\in M(A) \\cap M(B)) &= Pr(\\pi^{-1}(\\alpha) \\in S(A,w) \\cap S(B,w)) \\\\\n",
    "&= \\frac{|S(A,w) \\cap S(B,w)|}{|S(A,w) \\cup S(B,w)|}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we look at the second part, the intersection of M(A) and M(B).  We choose the minimum element from the previous term and call it &alpha; (actually, we can choose any element of the previous term and this still applies).  What is the probability that &alpha; is in the intersection of M(A) and M(B)?\n",
    "\n",
    "If an item is in the intersection, then it must be in M(A) __and__ M(B).  That implies that the element before the permutation was in both S(A,w) _and_ S(B,w). Here's the important leap in logic: __this probability is the same as the set resemblance__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- As before, suppose there are 10 elements in the unions of S(A) and S(B)\n",
    "- After permuting the union, every element in the union has an equal chance of being the minimum element\n",
    "- If the resemblance is 0.6 there's 60% chance that the minimum element in the permuted union is also the minimum element in the permuted intersection.\n",
    "- If it's the mininum element in the permuted intersection, it's the minimum element in both M(A) and M(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Again, the important things to note are (1) this is an _estimate_ not an exact value for the resemblance, and (2) we compute the estimate entirely from the document sketches.\n",
    "\n",
    "- Taking the minimum s elements is effectively like taking a random sample of size s.\n",
    "   - The larger s is, the closer we get to the true value of the resemblance\n",
    "   \n",
    "In later incarnations, Broder changes the methodology so that instead of taking s elements from the permutations, he instead make s permuations and saves the min value from each.  That's more in the spirit of current MinHash implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation Issues\n",
    "### Choosing a random permutation and a sample\n",
    "\n",
    "$$\n",
    "f : \\Omega \\rightarrow \\{ 0,...,2^l-1 \\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Despite the title of this section it starts with a discussion of basically compressing or hashing the shingles using a function _f_ that will map the shingle to a value between 0 and 2^l in order to reduce storage.  The resemblance is then said to be given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "r_w(A,B) = \\frac{|f(S(A,w)) \\cap f(S(B,w))|}{|f(S(A,w)) \\cup f(S(B,w))|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This notation is a bit confusing because in the first part of the section the paper talks about applying _f_ to a shingle, but in this formula f is being applied to a _set_ of shingles.  I'm assuming here that f(S(A,w)) means {f(shingle 1 of A),...,f(shingle w of A)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__ Fix and arbitrary set $ S \\in \\Omega $ of size n.  Then if f is chosen uniformly at random__\n",
    "$$\n",
    "E(|f(S)|) = 2^l(1-(1-\\frac{1}{2^l})^n) = n - \\binom{n}{2}\\frac{1}{2^l} + \\binom{n}{3}\\frac{1}{2^{2l}} + \\dots\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(|f(S)|) \\approx n - \\binom{n}{2}\\frac{1}{2^l}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The first equation is related to a variation of the _birthday problem_.  This version gives the expected value (mean) of the number of distinct values when choosing n values from a set of size 2<sup>l</sup>.  I won't go into the derivation of this, but here's a [good reference](http://www.math.uah.edu/stat/urn/Birthday.html).\n",
    "\n",
    "In the second equation Broder is basically saying that we can ignore the possibility of more than 2 shingles being mapped to the same value.  The main point that Broder is trying to make here is that there are likely to be few collisions when applying f to the shingles, so that size of the sets is nearly the same as the original shingles and the set resemblance formula will still work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Furthermore it can be argued that the size of f(S) is fairly well concentrated...__\n",
    "\n",
    "__...it is simpler to use Markov inequality__\n",
    "$$\n",
    "P(X \\ge a) \\le \\frac{E(X)}{a} \\ \\ \\text{(Markov inequality)}\n",
    "$$\n",
    "$$\n",
    "P(n - |f(S)| > \\binom{n}{2} \\frac{1}{2^{l-10}}) = \\frac{\\binom{n}{2}\\frac{1}{2^l}}{\\binom{n}{2}\\frac{1}{2^{l-10}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here Broder is just trying to show that the variation in the size of the set f(S) is fairly small.  Shown above is the Markov inequality, which shows that \"_the probability that the number of collisions exceeds $ \\binom{n}{2} \\frac{1}{2^{l-10}} $_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rabin Fingerprints\n",
    "\n",
    " - Represents strings as a polynomial\n",
    " - Basically a hash function but with lower collision probability\n",
    " - Can be be computed in a rolling/windowed fashion, which works well for the shingles\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Broder uses the Rabin fingerprint for f, both because the collision probablity is well understoond and because they can be computed efficiently for the shingles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Permutation\n",
    "\n",
    "__To produce our sample we need a random permutation $\\pi : \\{0,1,...,2^l\\} \\rightarrow  \\{0,1,...,2^l\\} $ __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Broder's a bit vague on how to generate the permutation, other than to say we need random one, or that we can basically treat the application of the Rabin fingerprint function to each shingle _as_ the permutation.  However, he does cover this in detail in a later paper on generating what he calls _min-wise independent_ permutations, which means any element in the original set has an equal probability of being the minimum element in the permutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The size of the sample\n",
    "\n",
    "$$\n",
    "p(s,r,\\epsilon) = \\sum\\limits_{s(r-\\epsilon)\\le k \\le s(r+\\epsilon)} \\binom{s}{k} r^k(1-r)^{s-k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this section Broder explains how to choose a sample (basically, the 's' in MINs).  The above formula shows how the probability that the resemblance calculated from the samples can be bounded by &epsilon; with some set probability if you choose the sample size to be large enough.\n",
    "\n",
    "For example, suppose that the true resemblance is 0.7.  If you had a sample of only 10, then the above formula says that the probability of the computed resemblance being between 0.6 and 0.8 is the sum of the binomial probabilities for 6,7, or 8 successes in 10 tries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7004233215\n",
      "0.971038739592\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "rv = binom(10, 0.7)\n",
    "print(sum(rv.pmf(k) for k in (6,7,8)))\n",
    "\n",
    "rv = binom(100, 0.7)\n",
    "print(sum(rv.pmf(k) for k in range(60,80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here you can see that a sample size of 100 gives a much higher probability of getting an estimate within the desired limits than a sample of 10.  Broder points out that with millions of documents, there will still be some cases where the sample is not big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating Resemblance\n",
    "\n",
    "- For any given pair, mergesort removing duplicates the already sorted \"sketches\" and count the number of duplicates in the first s, which is O(s)\n",
    "- Go through each sketch, and add to an existing cluster if the resemblance is close enough to a \"representative sketch\", or start a new cluster.\n",
    "- __It is likely in practice that each fingerprint belongs only to a few distinct clusters..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Broder describes a fairly standard clustering procedure, but doesn't give a lot of detail.  One of the key ideas is that fingerprints will only belong to a small number of clusters, so that as the number of clusters grows you don't have to check all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References and Other Sources\n",
    "\n",
    "- Andrei Z. Broder (1993). \"Some applications of Rabin's fingerprinting method\"\n",
    "-  Broder, Andrei Z.; Charikar, Moses; Frieze, Alan M.; Mitzenmacher, Michael (1998), \"Min-wise independent permutations\"\n",
    "- Michael O. Rabin (1981). \"Fingerprinting by Random Polynomials\"\n",
    "-  Indyk, Piotr.; Motwani, Rajeev. (1998). \"Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality.\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
